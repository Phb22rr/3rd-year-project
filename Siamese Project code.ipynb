{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ameoba CNN for the final project",
   "id": "a6ea1c082bcc18a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:42.360669Z",
     "start_time": "2025-03-04T18:30:25.257014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#save Ram from exploding\n",
    "#Begone memory\n",
    "def memory():\n",
    "    def Begone_memory(exceptions=None):\n",
    "        if exceptions is None:\n",
    "            exceptions = []\n",
    "\n",
    "        for var in list(globals().keys()):\n",
    "            if var not in exceptions:\n",
    "                del globals()[var]\n",
    "    Begone_memory()\n",
    "    def Begone_memory(exceptions=None):\n",
    "        if exceptions is None:\n",
    "            exceptions = []\n",
    "\n",
    "        for var in list(globals().keys()):\n",
    "            if var not in exceptions:\n",
    "                del globals()[var]\n",
    "memory()\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from openpyxl.styles.builtins import output\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from itertools import product\n",
    "from collections import OrderedDict, namedtuple\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import RunManager as rm\n",
    "import RunBuilder as bm\n",
    "\n",
    "# Utility Functions\n",
    "def imshow(img, text=None, should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic', fontweight='bold', bbox={'facecolor': 'white', 'alpha': 0.8, 'pad': 10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "def show_plot(iteration, loss):\n",
    "    plt.plot(iteration, loss)\n",
    "    plt.show()\n",
    "\n",
    "def sanitize_for_windows_path(name):\n",
    "    # Replace invalid characters with underscores or remove them\n",
    "    invalid_chars = r'<>:\"/\\\\|?*'\n",
    "    return ''.join('_' if c in invalid_chars else c for c in name)\n"
   ],
   "id": "a93ad49a368c6ad6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:42.378499Z",
     "start_time": "2025-03-04T18:30:42.373678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img1_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\img1.npy\"\n",
    "training_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\15.npz\"\n",
    "testing_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\2nd.npz\""
   ],
   "id": "3e9d08db52738c57",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:43.561991Z",
     "start_time": "2025-03-04T18:30:42.392507Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img1 = np.load(img1_directory)\n",
    "print(img1.shape)\n",
    "\n",
    "img2m = np.load(training_directory)\n",
    "img2 = img2m[\"images\"]\n",
    "labels = img2m[\"labels\"]\n",
    "print(img2.shape)\n",
    "print(np.mean(labels))"
   ],
   "id": "cd39436efa0aa95f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9244, 128, 128)\n",
      "(12619, 128, 128)\n",
      "0.47634519375544815\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:43.670196Z",
     "start_time": "2025-03-04T18:30:43.665939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 100x100 pixels # no we want 128x128 idiot, we like 128x128\n",
    "    transforms.ToTensor(),           # Convert to Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] # this does who knows what, probably jsut set's it up correctly\n",
    "])"
   ],
   "id": "d9afcb5791b7f9bd",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:43.788847Z",
     "start_time": "2025-03-04T18:30:43.782771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SiameseNetworkDataset():\n",
    "    def __init__(self, npz_file=None, transform=None): #load dataset from the provided .npz files\n",
    "        self.data = np.load(npz_file) #Extract images form loaded data\n",
    "        self.images = self.data['images'] #Extract labels from the loaded data\n",
    "        self.labels = self.data['labels'] #Extract labels form the loaded data\n",
    "        self.transform = transform #stroe any image transofmation(in .transform)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.images[index]\n",
    "        label = self.labels[index] # load the images with their associated label\n",
    "\n",
    "        img = Image.fromarray(img) # convert from numpy array to PIL image\n",
    "\n",
    "        #if self.transform: # apply trransofmation if one exists(one does)\n",
    "        #    img = self.transform(img)\n",
    "\n",
    "        return img, torch.tensor(label, dtype=torch.float32) # return the transformed image and label as a float tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images) #Return the total umber of images in the dataset"
   ],
   "id": "8d97987af650e4c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T18:30:43.839167Z",
     "start_time": "2025-03-04T18:30:43.830946Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SiameseNetworkDataset(torch.utils.data.Dataset): #\n",
    "    def __init__(self, img1_dir, img2_dir, transform=None): # preparing the data from the directories\n",
    "        self.img1_data = np.load(img1_dir)\n",
    "        self.img1_images = self.img1_data #loading in all the img1 images\n",
    "\n",
    "        self.img2_data = np.load(img2_dir) #loads the second image data\n",
    "        self.img2_images = self.img2_data['images'] #loads the images\n",
    "        self.img2_labels = self.img2_data['labels'] # loads the labels for img2\n",
    "\n",
    "        self.transform = transform # does some funky transformations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #sample a randomly healthy image from img1 (this is easy as the label is always 0 for healthy)\n",
    "        random_index = np.random.choice(len(self.img1_images)) # randomly loading an inividual image from img1\n",
    "        img1 = Image.fromarray(self.img1_images[random_index]).convert(\"L\") #opens the image as a greyscale(l stands for luminescence or however you spell it)\n",
    "\n",
    "        #time to load img2 and label directly from npz file(either healthy or a guy who visits maccies every day *cough* Niamh)\n",
    "        img2 = self.img2_images[index] # load the image numebred \"index\"\n",
    "        label = self.img2_labels[index] # loading the labels according to the image number/index thiongy\n",
    "\n",
    "        img2 = Image.fromarray(img2).convert(\"L\")  # make img 2 greyscale with some funky stuff I've never seen before but apparently it works\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)#transform img 1\n",
    "            img2 = self.transform(img2) # transform img 2\n",
    "\n",
    "        label = torch.tensor(float(label), dtype=torch.float32) # load label of img2 as a\n",
    "        return img1, img2, label # finish this tomfoolery and output this stuff\n",
    "    def __len__(self):#this is supposed to do something, I dont know\n",
    "        return len(self.img2_images)# ok, now I am even more confused, but the code says it works so lets keep it"
   ],
   "id": "f5ce8b13bace9660",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T19:40:30.825555Z",
     "start_time": "2025-03-04T18:30:43.848707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(1 # input channels (1 for greyscale)\n",
    "                      , 96 # output channels\n",
    "                      , kernel_size=11,stride=1), #\n",
    "            nn.ReLU(inplace=True), # acticate the function\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), # normalisation to stabalize training\n",
    "            nn.MaxPool2d(3, stride=2), # pooling to reduce spacial dimensions? whatever that means\n",
    "\n",
    "            nn.Conv2d(96, 256, kernel_size=5,stride=1,padding=2), # 2nd conv layer\n",
    "            nn.ReLU(inplace=True), # activate the layer\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2), # normalisation\n",
    "            nn.MaxPool2d(3, stride=2), # pooling\n",
    "            nn.Dropout2d(p=0.3), # dropout to reduceoverfitting\n",
    "\n",
    "            nn.Conv2d(256,384 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout2d(p=0.3), # all this stuff is the same as before\n",
    "\n",
    "        )\n",
    "\n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(43264, 1024), # input size depends onthe flattened CNN output\n",
    "            nn.ReLU(inplace=True), # activate the funciton\n",
    "            nn.Dropout(p=0.5), # dropout for rgulaisation\n",
    "\n",
    "            nn.Linear(1024, 128), # \"linearise\" it again\n",
    "            nn.ReLU(inplace=True), # activate it\n",
    "\n",
    "            nn.Linear(128, 1)) # originally the was the under bit of this line\n",
    "                      #,2))\n",
    "                            #last nn.linear is to get a distance similarity whatever that means\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        x1 = self.cnn1(img1) # pass both images through the first cnn layer\n",
    "        x2 = self.cnn1(img2)\n",
    "\n",
    "        x1 = x1.view(x1.size(0), -1) # flattten the outputs to a 2d tensor\n",
    "        x2 = x2.view(x2.size(0), -1)\n",
    "\n",
    "        distance = torch.abs(x1 - x2) # find the absolute difference between the 2 image results\n",
    "        score = self.fc1(distance) # put the distance into the fc layer to find a similarity score\n",
    "        output = torch.sigmoid(score)  # make the output either 0, or 1\n",
    "        return output"
   ],
   "id": "f183b2ad4feaa08e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-04T19:40:30.859763800Z",
     "start_time": "2025-03-04T18:30:43.869677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ContrastiveLoss, self).__init__() # margin defines minimuin distance for dissimilar pairs\n",
    "        self.bce = nn.BCELoss() # if simimilar pair in the marging hten loss is incurred\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        loss_contrastive = self.bce(output, label)\n",
    "        return loss_contrastive"
   ],
   "id": "908593f9b1290347",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:05:41.832434Z",
     "start_time": "2025-03-05T02:05:41.650272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Hyper Parameter search with RunManager\n",
    "params = OrderedDict( # params for the server\n",
    "    lr=[0.0001, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.1],\n",
    "    batch_size=[8, 16, 32, 64, 128, 256],\n",
    "    number_epochs=[20],\n",
    "    op=[torch.optim.AdamW, torch.optim.Adam, torch.optim.SGD,torch.optim.RAdam, torch.optim.NAdam])  # Only store the optimizer class here\n",
    "\n",
    "model = SiameseNetwork()\n",
    "\n",
    "m = rm.RunManager()\n",
    "b = bm.RunBuilder.get_runs(params)\n",
    "\n",
    "for run in b:\n",
    "    if run.op == torch.optim.SGD:\n",
    "        optimizer = run.op(model.parameters(), lr=run.lr, weight_decay=0.0005)\n",
    "    else:\n",
    "        optimizer = run.op(model.parameters(), lr=run.lr, eps=1e-8, weight_decay=0.0005)\n",
    "\n",
    "for run in b:\n",
    "    print(run)"
   ],
   "id": "6d510de7f1e336ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(lr=0.0001, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0001, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0001, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0001, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0001, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0001, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0001, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0001, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0001, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.001, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.001, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.001, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.001, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.001, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0025, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0025, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0025, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0025, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0025, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.005, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.005, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.005, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.005, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.005, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.0075, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.0075, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.0075, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.0075, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.0075, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.01, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.01, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.01, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.01, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.01, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=8, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=8, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=8, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=8, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=8, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=16, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=16, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=16, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=16, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=32, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=32, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=32, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=32, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=32, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=64, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=64, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=64, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=64, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=64, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=128, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=128, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=128, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=128, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=128, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n",
      "Run(lr=0.1, batch_size=256, number_epochs=20, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "Run(lr=0.1, batch_size=256, number_epochs=20, op=<class 'torch.optim.adam.Adam'>)\n",
      "Run(lr=0.1, batch_size=256, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "Run(lr=0.1, batch_size=256, number_epochs=20, op=<class 'torch.optim.radam.RAdam'>)\n",
      "Run(lr=0.1, batch_size=256, number_epochs=20, op=<class 'torch.optim.nadam.NAdam'>)\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:03:37.250758Z",
     "start_time": "2025-03-05T02:03:36.341202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define validation dataset\n",
    "val_dataset = SiameseNetworkDataset(img1_directory, #Directory containing the first set of images(anchor images)\n",
    "                                    testing_directory, # derecotry containing hte second set of images(comparision images)\n",
    "                                    transform=transforms.Compose([ #Defie image transformations\n",
    "                                        transforms.Resize((128, 128)), # resize to 128x128 pixels\n",
    "                                        transforms.ToTensor() # convert images to pytorch tensors\n",
    "                                    ]))\n",
    "\n",
    "# Create DataLoader using val_dataset\n",
    "validation_dataloader = DataLoader(val_dataset, # dataset to lead from (defined above)\n",
    "                                   shuffle=False, # No need to shefful for validation\n",
    "                                   num_workers=0, # Number of parralel processes(1 thread)\n",
    "                                   batch_size=run.batch_size, # Number of samples per batch, defined in training config\n",
    "                                   pin_memory=True) # pin memory to speed up data transfer to gpu (reccommended when using Cuda)"
   ],
   "id": "8312364c3ad42645",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:03:38.272380Z",
     "start_time": "2025-03-05T02:03:37.254850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset from raw image folders\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(\n",
    "    img1_dir=r\"/data/Siamese_dataset/img1.npy\", #Directory containing the first set of images(anchor images)\n",
    "    img2_dir=training_directory, # derecotry containing hte second set of images(comparision images)\n",
    "    transform=transforms.Compose([ #Defie image transformations\n",
    "        transforms.Resize((128, 128)), # resize to 128x128 pixels\n",
    "        transforms.ToTensor()])) # convert images to pytorch tensors\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(siamese_dataset, # loading inthe siamese dataset defined above\n",
    "                              shuffle=True, #shuffle for training to randomise things\n",
    "                              num_workers=0, # no. parralllel processes\n",
    "                              batch_size=run.batch_size, # set a batch size\n",
    "                              pin_memory= True) # pinmemory to speed up data transfer to the gpu"
   ],
   "id": "1d734ec886d47978",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:03:38.288360Z",
     "start_time": "2025-03-05T02:03:38.281248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Hyperparameter Search with RunManager and Run Builder\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        runs = []\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        return runs\n",
    "\n",
    "\n",
    "class RunManager():\n",
    "    def save_checkpoint(self, model, optimizer, epoch, filename=\"checkpoint.pth\"):\n",
    "        \"\"\"Save model checkpoint at the current epoch.\"\"\"\n",
    "        print(f\"Saving checkpoint at epoch {epoch}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }, filename)\n",
    "\n",
    "    def validate(self, model, validation_dataloader):\n",
    "        model.eval()  # Switch to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss_history = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for data in validation_dataloader:\n",
    "                img1, img2, label = data\n",
    "                img1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "\n",
    "                output = model(img1, img2).squeeze(1)\n",
    "                loss_contrastive = criterion(output, label)\n",
    "                loss_history.append(loss_contrastive.item())\n",
    "\n",
    "                # Compute accuracy#\n",
    "                #euclidean_distance = F.pairwise_distance(output)\n",
    "                predictions = (output < 0.5).float()\n",
    "                correct += (predictions == label).sum().item()\n",
    "                total += label.size(0)\n",
    "\n",
    "        accuracy = correct / total\n",
    "        avg_loss = sum(loss_history) / len(loss_history)\n",
    "        print(f\"Validation Accuracy: {accuracy*100:.4f}, Avg. Validation Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        return accuracy, avg_loss"
   ],
   "id": "4a4027ec5224a12b",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:03:38.982978Z",
     "start_time": "2025-03-05T02:03:38.974712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#def train():\n",
    "def train(net, train_loader, criterion, optimizer, device, number_epochs = run.number_epochs):\n",
    "    total_batches_seen = 1\n",
    "    total_batches = len(train_dataloader) * number_epochs\n",
    "    start_train = time.time()\n",
    "\n",
    "    net.to(device) # movethe model to the device\n",
    "\n",
    "    #for final graph\n",
    "    epoch_number = 1\n",
    "    counter = []\n",
    "    loss_history = []\n",
    "    epoch_acc = []\n",
    "\n",
    "    optimizer = run.op(net.parameters(), lr=run.lr) # initialise the optimizer from the run object\n",
    "\n",
    "    for epoch in range(number_epochs): # loop over all epochs\n",
    "        total_samples = 0\n",
    "\n",
    "        net.train() # set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "\n",
    "        for batch_idx,(img1, img2, label) in enumerate(train_dataloader): # get a pair of images and their similatiry, 0 for same, 1 for differnet, label is the label for img2\n",
    "            current_iter = epoch * len(train_dataloader) + batch_idx + 1\n",
    "            total_iter = len(train_dataloader) * number_epochs\n",
    "\n",
    "            img1, img2, label = img1.cuda(), img2.cuda(), label.cuda() # load to gpu\n",
    "\n",
    "            optimizer.zero_grad() # resets gradients for this batch\n",
    "            output = net(img1, img2).squeeze(1) # output should be similarity score (ideally 0 = similar, 1 = different)\n",
    "            loss_contrastive = criterion(output, label) # find loss between predicted similarity score and true label\n",
    "            loss_contrastive.backward()  #backpropogate the loss to compute the gradients\n",
    "            optimizer.step() # update modeel parameters using computated gradients\n",
    "\n",
    "            running_loss += loss_contrastive.item() # track the running loss\n",
    "\n",
    "            end_batch = time.time()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predicted = (output < 0.5).float()  # similarity scores < 0.5 = similar (label 0)\n",
    "            correct += (predicted == label).sum().item()  # count how many predictions matched the label\n",
    "            accuracy1 = correct / total_batches_seen\n",
    "\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "            if total_batches_seen % 10000 == 0 or total_batches_seen in{10} :\n",
    "                elapsed_time = end_batch - start_train\n",
    "                mean_time_per_batch = elapsed_time / total_batches_seen\n",
    "                batches_left = total_batches - (batch_idx + 1 + epoch * len(train_dataloader))\n",
    "\n",
    "                progress_fraction =current_iter / total_iter\n",
    "\n",
    "                remaining_time = batches_left * mean_time_per_batch\n",
    "                hours = int(remaining_time // 3600)\n",
    "                minutes = int((remaining_time % 3600) // 60)\n",
    "                seconds = int(remaining_time % 60)\n",
    "\n",
    "                print(f\"We are {progress_fraction * 100:.2f}% complete. ETA: {hours}h {minutes}m {seconds}s\")\n",
    "            total_batches_seen += 1\n",
    "\n",
    "        # Compute and store accuracy for the epoch\n",
    "        epoch_acc.append(accuracy1 * 100)\n",
    "        counter.append(epoch_number)\n",
    "        epoch_number += 1#add one to the current value of epoch_number\n",
    "        loss_history.append(running_loss)\n",
    "\n",
    "    plt.plot(counter, loss_history)\n",
    "    plt.show()\n",
    "    return net, counter, loss_history, epoch_acc #return trained model after all epochs are complete"
   ],
   "id": "38a10741a82bfd63",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T02:04:02.864044Z",
     "start_time": "2025-03-05T02:03:45.804682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#imitialiseparameters to trackbest model and corresponding optimiser\n",
    "best_validation_accuracy = 0.0#track highest validation accuracy seen\n",
    "best_model = None #stores best perfoming model\n",
    "best_optimizer = None # stores optimiser for said model\n",
    "\n",
    "m = RunManager()\n",
    "\n",
    "net = SiameseNetwork().cuda() # create fresh instance fo the Siamese netowrk and move to gpu\n",
    "criterion = ContrastiveLoss() # set up loss function(stays same for all runs\n",
    "\n",
    "#loop through all hyperparameter combinations\n",
    "for run in b:\n",
    "    start_whole_run = time.time()\n",
    "    print(f\"Running hyperparameter set: {run}\") # print the hyperparameters being used\n",
    "\n",
    "    # Create DataLoader with the specified batch size\n",
    "    train_dataloader = torch.utils.data.DataLoader(siamese_dataset, batch_size=run.batch_size, shuffle=True) #create the training set usingthe hyperparameters\n",
    "    validation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=run.batch_size, shuffle=False)#create calidation set for curent hyperparameters\n",
    "\n",
    "    # Initialize model and optimizer\n",
    "    model = SiameseNetwork()\n",
    "    model = model.cuda()  # Move to GPU if available\n",
    "    optimizer = run.op(model.parameters(), lr=run.lr)\n",
    "\n",
    "    # Train the model\n",
    "    model, counter, loss_history, epoch_acc = train(model, train_dataloader, criterion, optimizer, device = torch.device(\"cuda:0\"))\n",
    "\n",
    "    # Validate the model\n",
    "    accuracy, avg_loss = m.validate(model, validation_dataloader) # should return the accuracy and average loss for the validation data\n",
    "\n",
    "    # Save the best model\n",
    "    if accuracy > best_validation_accuracy: # if current model better than previous model then append to\n",
    "        best_validation_accuracy = accuracy\n",
    "        best_model = model\n",
    "        best_optimizer = optimizer\n",
    "        filename = f\"best_model_run_lr{run.lr}_bs{run.batch_size}_epochs{run.number_epochs}.pth\"\n",
    "        m.save_checkpoint(model, optimizer, run.number_epochs, filename=filename) # save the best performing models checkpoint\n",
    "\n",
    "    end_whole_run = time.time()\n",
    "\n",
    "print(f\"Best Validation Accuracy: {best_validation_accuracy:.4f}\")"
   ],
   "id": "6c00bada777f0b54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyperparameter set: Run(lr=0.001, batch_size=16, number_epochs=50, op=<class 'torch.optim.adamw.AdamW'>)\n",
      "We are 0.03% complete. ETA: 11h 34m 17s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[30], line 26\u001B[0m\n\u001B[0;32m     23\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m run\u001B[38;5;241m.\u001B[39mop(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mrun\u001B[38;5;241m.\u001B[39mlr)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m model, counter, loss_history, epoch_acc \u001B[38;5;241m=\u001B[39m train(model, train_dataloader, criterion, optimizer, device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda:0\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Validate the model\u001B[39;00m\n\u001B[0;32m     29\u001B[0m accuracy, avg_loss \u001B[38;5;241m=\u001B[39m m\u001B[38;5;241m.\u001B[39mvalidate(model, validation_dataloader) \u001B[38;5;66;03m# should return the accuracy and average loss for the validation data\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[29], line 36\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(net, train_loader, criterion, optimizer, device, number_epochs)\u001B[0m\n\u001B[0;32m     33\u001B[0m loss_contrastive\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m#backpropogate the loss to compute the gradients\u001B[39;00m\n\u001B[0;32m     34\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep() \u001B[38;5;66;03m# update modeel parameters using computated gradients\u001B[39;00m\n\u001B[1;32m---> 36\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss_contrastive\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;66;03m# track the running loss\u001B[39;00m\n\u001B[0;32m     38\u001B[0m end_batch \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     40\u001B[0m \u001B[38;5;66;03m# Calculate accuracy\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7630709bd0f4cd61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
