{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Only difference between this code and the actual Siamese Project code.ipynb is:\n",
    "cell 9 row 1: replaced img1_directory with training_directory\n",
    "cell 9 row 10: replaced img1_directory with testing_directory"
   ],
   "id": "2e1f8b66f7ba4cb2"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:44.831868Z",
     "start_time": "2025-03-09T21:10:44.827303Z"
    }
   },
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from itertools import product\n",
    "from collections import OrderedDict, namedtuple\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "def sanitize_for_windows_path(name):\n",
    "\tinvalid_chars = r'<>:\"/\\\\|?*'\n",
    "\treturn ''.join('_' if c in invalid_chars else c for c in name)"
   ],
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:45.461704Z",
     "start_time": "2025-03-09T21:10:45.458160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "img1_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\img1.npz\"\n",
    "training_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\15.npz\"\n",
    "testing_directory = \"A:\\\\3rd_Year_Project\\\\Project_code\\\\data\\\\Siamese_dataset\\\\2nd.npz\""
   ],
   "id": "eb5b5f8aeaf7d94d",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:45.839922Z",
     "start_time": "2025-03-09T21:10:45.835923Z"
    }
   },
   "cell_type": "code",
   "source": "transform = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])",
   "id": "c7133b52ab22d3f1",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:46.217880Z",
     "start_time": "2025-03-09T21:10:46.211952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SiameseNetworkDataset():\n",
    "\tdef __init__(self, npz_file = None, transform = None):\n",
    "\t\tself.data = np.load(npz_file)\n",
    "\t\tself.images = self.data['images']\n",
    "\t\tself.labels = self.data['labels']\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef get__item__(self, index):\n",
    "\t\timg = self.images[index]\n",
    "\t\tlabel = self.labels[index]\n",
    "\t\timg = Image.fromarray(img)\n",
    "\t\treturn img, torch.tensor(label, dtype= torch.float32)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.images)\n",
    "\n",
    "class SiameseNetworkDataset(torch.utils.data.Dataset):\n",
    "\tdef __init__(self, img1_dir, img2_dir, transform = None):\n",
    "\t\tself.img1_data = np.load(img1_dir)\n",
    "\t\tself.img1_images = self.img1_data['images']\n",
    "\t\tself.img1_labels = self.img1_data['labels']\n",
    "\n",
    "\t\tself.img2_data = np.load(img2_dir)\n",
    "\t\tself.img2_images = self.img2_data['images']\n",
    "\t\tself.img2_labels = self.img2_data['labels']\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\trandom_index = np.random.choice(len(self.img1_images))\n",
    "\t\timg1 = Image.fromarray(self.img1_images[random_index]).convert(\"L\")\n",
    "\n",
    "\t\timg2 = self.img2_images[index]\n",
    "\t\timg2 = Image.fromarray(img2).convert(\"L\")\n",
    "\n",
    "\t\tif self.img1_labels[index] == self.img2_labels[index]:\n",
    "\t\t\tlabel = 0\n",
    "\t\telse:\n",
    "\t\t\tlabel = 1\n",
    "\n",
    "\t\tif self.transform:\n",
    "\t\t\timg1 = self.transform(img1)\n",
    "\t\t\timg2 = self.transform(img2)\n",
    "\t\tlabel = torch.tensor(float(label), dtype=torch.float32)\n",
    "\t\treturn img1, img2, label\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.img2_images)"
   ],
   "id": "37f36f171f38712a",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:46.532849Z",
     "start_time": "2025-03-09T21:10:46.527058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SiameseNetwork, self).__init__()\n",
    "\t\tself.cnn1 = nn.Sequential(nn.Conv2d(1, 96,\n",
    "\t\t\t\t\t\t\t\t\t\t\tkernel_size=11, stride=1),\n",
    "\t\t\t\t\t\t\t\t  nn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\t\t\t  nn.LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2),\n",
    "\t\t\t\t\t\t\t\t  nn.MaxPool2d(3, stride=2),\n",
    "\t\t\t\t\t\t\t\t  nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "\t\t\t\t\t\t\t\t  nn.ReLU(inplace=True), nn.LocalResponseNorm(5, alpha=0.001, beta=0.75, k=2),\n",
    "\t\t\t\t\t\t\t\t  nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3),\n",
    "\t\t\t\t\t\t\t\t  nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\t\t\t\t\t\t  nn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\t\t\t  nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "\t\t\t\t\t\t\t\t  nn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\t\t\t  nn.MaxPool2d(3, stride=2), nn.Dropout2d(p=0.3), )\n",
    "\n",
    "\t\tself.fc1 = nn.Sequential(nn.Linear(43264, 1024), nn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\t\t\t nn.Dropout(p=0.5),\n",
    "\t\t\t\t\t\t\t\t nn.Linear(1024, 128),\n",
    "\t\t\t\t\t\t\t\t nn.ReLU(inplace=True),\n",
    "\t\t\t\t\t\t\t\t nn.Linear(128, 1))\n",
    "\n",
    "\tdef forward(self, img1, img2):\n",
    "\t\tx1 = self.cnn1(img1)\n",
    "\t\tx2 = self.cnn1(img2)\n",
    "\t\tx1 = x1.view(x1.size(0), -1)\n",
    "\t\tx2 = x2.view(x2.size(0), -1)\n",
    "\t\tdistance = torch.abs(x1 - x2)\n",
    "\t\tscore = self.fc1(distance)\n",
    "\t\toutput = torch.sigmoid(score)\n",
    "\t\treturn output"
   ],
   "id": "a68b6bddf41968d",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:46.985481Z",
     "start_time": "2025-03-09T21:10:46.982495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ContrastiveLoss, self).__init__()\n",
    "\t\tself.bce = nn.BCELoss()\n",
    "\n",
    "\tdef forward(self, output, label):\n",
    "\t\tloss_contrastive = self.bce(output, label)\n",
    "\t\treturn loss_contrastive"
   ],
   "id": "a368e28c00ab2f36",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:47.549597Z",
     "start_time": "2025-03-09T21:10:47.544095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RunBuilder():\n",
    "\t@staticmethod\n",
    "\tdef get_runs(params):\n",
    "\t\tRun = namedtuple('Run', params.keys())\n",
    "\t\truns = []\n",
    "\t\tfor v in product(*params.values()):\n",
    "\t\t\truns.append(Run(*v))\n",
    "\t\treturn runs\n",
    "\n",
    "\n",
    "class RunManager():\n",
    "\tdef save_checkpoint(self, model, optimizer, epoch, filename=\"checkpoint.pth\"):\n",
    "\t\tprint(f\"Saving checkpoint at epoch {epoch}\")\n",
    "\t\ttorch.save(\n",
    "\t\t\t{'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), },\n",
    "\t\t\tfilename)\n",
    "\n",
    "\tdef validate(self, model, validation_dataloader):\n",
    "\t\tmodel.eval()\n",
    "\t\tcorrect = 0\n",
    "\t\ttotal = 0\n",
    "\t\tloss_history = []\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor data in validation_dataloader:\n",
    "\t\t\t\timg1, img2, label = data\n",
    "\t\t\t\timg1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "\t\t\t\toutput = model(img1, img2).squeeze(1)\n",
    "\t\t\t\tloss_contrastive = criterion(output, label)\n",
    "\t\t\t\tloss_history.append(loss_contrastive.item())\n",
    "\t\t\t\tpredictions = (output < 0.5).float()\n",
    "\t\t\t\tcorrect += (predictions == label).sum().item()\n",
    "\t\t\t\ttotal += label.size(0)\n",
    "\t\taccuracy = correct / total\n",
    "\t\tavg_loss = sum(loss_history) / len(loss_history)\n",
    "\t\treturn accuracy, avg_loss"
   ],
   "id": "d74c917a8880c25c",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:48.157377Z",
     "start_time": "2025-03-09T21:10:47.969094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params = OrderedDict(lr=[0.0001],\n",
    "\t\t\t\t\t batch_size=[16],\n",
    "\t\t\t\t\t number_epochs=[20],\n",
    "\t\t\t\t\t op=[torch.optim.SGD])\n",
    "model = SiameseNetwork()\n",
    "m = RunManager()\n",
    "b = RunBuilder.get_runs(params)\n",
    "for run in b:\n",
    "\tif run.op == torch.optim.SGD:\n",
    "\t\toptimizer = run.op(model.parameters(), lr=run.lr, weight_decay=0.0005)\n",
    "\telse:\n",
    "\t\toptimizer = run.op(model.parameters(), lr=run.lr, eps=1e-8, weight_decay=0.0005)"
   ],
   "id": "ab388fa9b7dc802f",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:51.358516Z",
     "start_time": "2025-03-09T21:10:48.468971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "siamese_dataset = SiameseNetworkDataset(img1_dir=training_directory,\n",
    "\t\t\t\t\t\t\t\t\t\timg2_dir=training_directory,\n",
    "\t\t\t\t\t\t\t\t\t\ttransform=transforms.Compose(\n",
    "\t\t\t\t\t\t\t\t\t\t\t[transforms.Resize((128, 128)), transforms.ToTensor()]))\n",
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "\t\t\t\t\t\t\t  shuffle=True,\n",
    "\t\t\t\t\t\t\t  num_workers=0,\n",
    "\t\t\t\t\t\t\t  batch_size=run.batch_size,\n",
    "\t\t\t\t\t\t\t  pin_memory=True)\n",
    "val_dataset = SiameseNetworkDataset(testing_directory,\n",
    "\t\t\t\t\t\t\t\t\ttesting_directory,\n",
    "\t\t\t\t\t\t\t\t\ttransform=transforms.Compose(\n",
    "\t\t\t\t\t\t\t\t\t\t[transforms.Resize((128, 128)), transforms.ToTensor()]))\n",
    "validation_dataloader = DataLoader(val_dataset,\n",
    "\t\t\t\t\t\t\t\t   shuffle=False,\n",
    "\t\t\t\t\t\t\t\t   num_workers=0,\n",
    "\t\t\t\t\t\t\t\t   batch_size=run.batch_size,\n",
    "\t\t\t\t\t\t\t\t   pin_memory=True)"
   ],
   "id": "ba8ad753c4c20506",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T21:10:51.371059Z",
     "start_time": "2025-03-09T21:10:51.362053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train(net, train_loader, criterion, optimizer, device, number_epochs=run.number_epochs):\n",
    "\ttotal_batches_seen = 1\n",
    "\ttotal_batches = len(train_dataloader) * number_epochs\n",
    "\tnet.to(device)\n",
    "\tepoch_number = 1\n",
    "\tcounter = []\n",
    "\tloss_history = []\n",
    "\tepoch_acc = []\n",
    "\toptimizer = run.op(net.parameters(), lr=run.lr)\n",
    "\tfor epoch in range(number_epochs):\n",
    "\t\ttotal_samples = 0\n",
    "\t\tnet.train()\n",
    "\t\trunning_loss = 0.0\n",
    "\t\tcorrect = 0\n",
    "\t\tfor batch_idx, (img1, img2, label) in enumerate(train_dataloader):\n",
    "\t\t\tcurrent_iter = epoch * len(train_dataloader) + batch_idx + 1\n",
    "\t\t\timg1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\toutput = net(img1, img2).squeeze(1)\n",
    "\t\t\tloss_contrastive = criterion(output, label)\n",
    "\t\t\tloss_contrastive.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\trunning_loss += loss_contrastive.item()\n",
    "\t\t\tpredicted = (output < 0.5).float()\n",
    "\t\t\tcorrect += (predicted == label).sum().item()\n",
    "\t\t\taccuracy1 = correct / total_batches_seen\n",
    "\t\t\ttotal_samples += label.size(0)\n",
    "\t\t\tif total_batches_seen % 100 == 0:\n",
    "\t\t\t\tend_batch = time.time()\n",
    "\t\t\t\ttime_training = end_batch - start_whole_run\n",
    "\n",
    "\t\t\t\tprogress_fraction = current_iter / total_batches\n",
    "\t\t\t\tpercent_complete = ((total_parameters_trained + progress_fraction - 1) / total_parameters) * 100\n",
    "\n",
    "\t\t\t\tpercentage_left = 100 - percent_complete\n",
    "\t\t\t\ttime_remaining = (percentage_left / percent_complete) * time_training\n",
    "\n",
    "\t\t\t\tdays1 = int(time_remaining // 86400)\n",
    "\t\t\t\thours1 = int(time_remaining % 86400 // 3600)\n",
    "\t\t\t\tminutes1 = int((time_remaining % 3600) // 60)\n",
    "\t\t\t\tseconds1 = int(time_remaining % 60)\n",
    "\n",
    "\t\t\t\tprint(  #f\"{progress_fraction * 100:.2f}% complete of cell.\",\n",
    "\t\t\t\t\tf\"run {percent_complete:.4f}% complete\",\n",
    "\t\t\t\t\tf\"Total ETA: {days1}d {hours1}h {minutes1}m {seconds1}s\")\n",
    "\t\t\ttotal_batches_seen += 1\n",
    "\n",
    "\t\tepoch_acc.append(accuracy1 * 100)\n",
    "\t\tcounter.append(epoch_number)\n",
    "\t\tepoch_number += 1\n",
    "\t\tloss_history.append(running_loss)\n",
    "\n",
    "    # Ensure indentation is correct here\n",
    "\tplt.xticks(range(min(counter), max(counter) + 1))  # X-axis integer ticks\n",
    "\t#plt.yticks(range(min(y), max(y) + 1, 5))  # Y-axis integer ticks (step of 5)\n",
    "\tplt.plot(counter, loss_history, label=\"Training Loss\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.legend()\n",
    "\n",
    "\treturn net, counter, loss_history, epoch_acc  # Ensure return is correctly aligned"
   ],
   "id": "2740523d45204de1",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-09T21:10:51.381098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_validation_accuracy = 0.0\n",
    "best_model = None\n",
    "best_optimizer = None\n",
    "m = RunManager()\n",
    "net = SiameseNetwork().cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "\n",
    "start_whole_run = time.time()\n",
    "total_parameters = len(b)\n",
    "total_parameters_trained = 0\n",
    "Epoch_num = []\n",
    "ETA_list = []\n",
    "Accuracy_list = []\n",
    "Loss_list = []\n",
    "learning_rate_list = []\n",
    "Batch_size_list = []\n",
    "days1 = 0\n",
    "hours1 = 0\n",
    "minutes1 = 0\n",
    "seconds1 = 0\n",
    "end_parameter_set = 0\n",
    "\n",
    "for run in b:\n",
    "\ttorch.cuda.empty_cache()\n",
    "\ttotal_parameters_trained += 1\n",
    "\tprint(f\"Running hyper parameters set:{run}\")\n",
    "\ttrain_dataloader = torch.utils.data.DataLoader(siamese_dataset, batch_size=run.batch_size, shuffle=True)\n",
    "\tvalidation_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=run.batch_size, shuffle=False)\n",
    "\tmodel = SiameseNetwork()\n",
    "\tmodel = model.cuda()\n",
    "\toptimizer = run.op(model.parameters(), lr=run.lr)\n",
    "\tpre_train_time = time.time()\n",
    "\tmodel, counter, loss_history, epoch_acc = train(model, train_dataloader, criterion, optimizer,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\tdevice=torch.device(\"cuda:0\"))\n",
    "\tpost_train_time = time.time()\n",
    "\ttotal_time = post_train_time - pre_train_time\n",
    "\taccuracy, avg_loss = m.validate(model, validation_dataloader)\n",
    "\tEpoch_num.append(run.number_epochs)\n",
    "\tETA_list.append(total_time)\n",
    "\tAccuracy_list.append(accuracy)\n",
    "\tLoss_list.append(avg_loss)\n",
    "\tBatch_size_list.append(run.batch_size)\n",
    "\tlearning_rate_list.append(run.lr)\n",
    "\tif accuracy > best_validation_accuracy:\n",
    "\t\tbest_validation_accuracy = accuracy\n",
    "\t\tbest_model = model\n",
    "\t\tbest_optimizer = optimizer\n",
    "\t\tfilename = f\"BestRun_lr{run.lr}_bs{run.batch_size}_epoch{run.number_epochs}_op{run.op}_acc{(best_validation_accuracy * 100):.0f}.pth\"\n",
    "\t\tm.save_checkpoint(model, optimizer, run.number_epochs, filename=filename)\n",
    "\t\tprint(run, \"Is the new best run. \", f\"Accuracy:{(best_validation_accuracy * 100):.2f}\")\n",
    "\n",
    "\tend_parameter_set = time.time()\n",
    "\n",
    "headers = [\"Epoch Number\", \"ETA\", \"Accuracy\", \"Loss\", \"Learning Rate\", \"Batch Size\"]\n",
    "csv_path = \"Optimisation data.csv\"\n",
    "rows = zip(Epoch_num, ETA_list, Accuracy_list, Loss_list, learning_rate_list, Batch_size_list)\n",
    "with open(csv_path, \"w\", newline=\"\") as file:\n",
    "\twriter = csv.writer(file)\n",
    "\twriter.writerow(headers)\n",
    "print(\",\".join(headers))\n",
    "for row in rows:\n",
    "\tprint(\",\".join(map(str, row)))"
   ],
   "id": "2ea8d04a7751597c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyper parameters set:Run(lr=0.0001, batch_size=16, number_epochs=20, op=<class 'torch.optim.sgd.SGD'>)\n",
      "run 0.6337% complete Total ETA: 0d 1h 29m 2s\n",
      "run 1.2674% complete Total ETA: 0d 1h 27m 25s\n",
      "run 1.9011% complete Total ETA: 0d 1h 27m 21s\n",
      "run 2.5349% complete Total ETA: 0d 1h 27m 19s\n",
      "run 3.1686% complete Total ETA: 0d 1h 26m 28s\n",
      "run 3.8023% complete Total ETA: 0d 1h 25m 47s\n",
      "run 4.4360% complete Total ETA: 0d 1h 25m 3s\n",
      "run 5.0697% complete Total ETA: 0d 1h 24m 19s\n",
      "run 5.7034% complete Total ETA: 0d 1h 23m 48s\n",
      "run 6.3371% complete Total ETA: 0d 1h 23m 12s\n",
      "run 6.9708% complete Total ETA: 0d 1h 22m 40s\n",
      "run 7.6046% complete Total ETA: 0d 1h 22m 3s\n",
      "run 8.2383% complete Total ETA: 0d 1h 21m 28s\n",
      "run 8.8720% complete Total ETA: 0d 1h 20m 53s\n",
      "run 9.5057% complete Total ETA: 0d 1h 20m 26s\n",
      "run 10.1394% complete Total ETA: 0d 1h 19m 58s\n",
      "run 10.7731% complete Total ETA: 0d 1h 19m 30s\n",
      "run 11.4068% complete Total ETA: 0d 1h 19m 2s\n",
      "run 12.0406% complete Total ETA: 0d 1h 18m 31s\n",
      "run 12.6743% complete Total ETA: 0d 1h 17m 57s\n",
      "run 13.3080% complete Total ETA: 0d 1h 17m 32s\n",
      "run 13.9417% complete Total ETA: 0d 1h 17m 3s\n",
      "run 14.5754% complete Total ETA: 0d 1h 16m 28s\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2822ca91fabd269f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
